### [[3.5: Evaluating changes.]]

Now that we have established how to set up Attribute Axes, Priorities and create Axioms we can start to look at how to make evaluate changes, before making them and afterwards.

Let's start by creating an example problem, by setting up the environment for it, current state, and our goals (end state).

Let's start with a simple website with some dynamic content, which we are deploying.

Our goal is to improve our performance in doing the deployment.  Currently, we are copying the installation files sequentially from a single server, and we have 100 web or application servers which we are copying them to (we will call them "web" servers for simplicity).

The exact mechanisms for copying and deployment are not going to be the focus of this example, and so we won't be evaluating their performance or impacts, because it will complicate the example.  In the real world, of course, these factors are also important, and would have be evaluated independently, and in conjuncture with the rest of the changes.  

It is important that all details are "Aligned" in that they work well together, and perform efficient as a whole (in their sequential and parallel processing), over long periods of time, and under our given resiliency goals.  Alignment is complicated to explain, so I will keep giving examples of it over time, so that you can build up your own understanding of the concept, and how to apply the term as I am using it.

So, we have a current state:

- Single server, connects to 100 servers over SCP (secure copy) sequentially.

What are some options that will perform better than running 100 sequential copies?

One method is running the copies on the same single server, but running the copy commands in parallel instead of sequentially.

There are a number of options for running things in parallel:

- Forking the process, so that there are many processes of SCP running.  Forked processes are independent of the process that forked them (they start as clones, and then do their custom thing), and do not have a lot of communication with the parent.  There are ways to know some things about the program though, so this is a viable option.

- Using a controller program to create Threads, and running the SCP process in the threads (similar to forking, in that there are still 100 (in this case) SCP processes being run, but they are being controlled by a single program, instead of just independently launching.

Since I want to constrain this example, I won't get into the differences between controlling forked and threaded sub-processes, it's enough to know that while they have differences, we can get the results we want out of either of them for this case.

{{ todo__describe_forked_vs_threaded_somewhere_else }}

It doesn't matter whether we write our own code to do this forked or thread handling, or we use software someone else made, such as "Orchestration" software which might have agents that run on each target server, and perform the copy in it's own way.

So we have specified 2 methods running on the same single source server, and performing the work in parallel.

Since we have run into this performance problem by running things sequentially, one thing we will do is say, "Let's not get into this situation again, once our server targets have grown again", and say that we are removing the option of using sequential copies again, because it will not scale, and will create work for us again in the future.

Since we just went over how create an Axiom, let's do it now.  However, since we are working with a specific scenario and have not yet looked at all-of-operations together, we shouldn't make this a Universal Axiom, or a Production Axiom.

Instead, let's make it a Working Axiom, or a Temporary Axiom, for the purpose of this problem.

Here goes:

"Once a job that was working sequentially on a single server hits a scaling problem, and needs to be changed for performance, all later implementations of that job will not be re-implemented sequentially, even on more than a single server."

This is a little longer than I like for an Axiom, and it has more caveats than I prefer as well, as I like them simple and straight-forward, always able to be applied.

In this circumstance, we are building a Working Axiom, so it is temporary, and it needs to be precise, so it needs those caveats to fit into our problem space.

Let's review the verbiage quickly to see what caveats I baked in:

- I have limited this to implementations that already exist, and were implemented as sequential processing, on a single instance.

This could actually apply to a number of different scenarios, not just the one we are currently discussing, so this is good Axiom material, as we want to be able to apply Axioms generally, so that they are usable or actionable.

- I specify that this axiom is only to be applied once the work hits a problem.  Going around and "fixing" problems that aren't problems is a poor use of resources, as there are likely actual problems that need fixing.  Also, it is creating change in the environment, which may lead to instability or outages.  

All changes come with risk, as you only know the effects of your previous system from your history with it, once you change it you are working with a new system (new state vs. old state), so you may get different effects.  

If your change is implemented ideally, then the effects that change are exactly what you planned and wanted, so everything is OK.  If you get effects you did not want, or did not anticipate, you will likely be created a problem.
