### [[3.5: Evaluating changes.]]

Now that we have established how to set up Attribute Axes, Priorities and create Axioms we can start to look at how to make evaluate changes, before making them and afterwards.

Let's start by creating an example problem, by setting up the environment for it, current state, and our goals (end state).

Let's start with a simple website with some dynamic content, which we are deploying.

Our goal is to improve our performance in doing the deployment.  Currently, we are copying the installation files sequentially from a single server, and we have 100 web or application servers which we are copying them to (we will call them "web" servers for simplicity).

The exact mechanisms for copying and deployment are not going to be the focus of this example, and so we won't be evaluating their performance or impacts, because it will complicate the example.  In the real world, of course, these factors are also important, and would have be evaluated independently, and in conjuncture with the rest of the changes.  

It is important that all details are "Aligned" in that they work well together, and perform efficient as a whole (in their sequential and parallel processing), over long periods of time, and under our given resiliency goals.  Alignment is complicated to explain, so I will keep giving examples of it over time, so that you can build up your own understanding of the concept, and how to apply the term as I am using it.

So, we have a current state:

- Single server, connects to 100 servers over SCP (secure copy) sequentially.

What are some options that will perform better than running 100 sequential copies?

One method is running the copies on the same single server, but running the copy commands in parallel instead of sequentially.

There are a number of options for running things in parallel:

- Forking the process, so that there are many processes of SCP running.  Forked processes are independent of the process that forked them (they start as clones, and then do their custom thing), and do not have a lot of communication with the parent.  There are ways to know some things about the program though, so this is a viable option.

- Using a controller program to create Threads, and running the SCP process in the threads (similar to forking, in that there are still 100 (in this case) SCP processes being run, but they are being controlled by a single program, instead of just independently launching.

Since I want to constrain this example, I won't get into the differences between controlling forked and threaded sub-processes, it's enough to know that while they have differences, we can get the results we want out of either of them for this case.

{{ todo__describe_forked_vs_threaded_somewhere_else }}

It doesn't matter whether we write our own code to do this forked or thread handling, or we use software someone else made, such as "Orchestration" software which might have agents that run on each target server, and perform the copy in it's own way.

So we have specified 2 methods running on the same single source server, and performing the work in parallel.

Since we have run into this performance problem by running things sequentially, one thing we will do is say, "Let's not get into this situation again, once our server targets have grown again", and say that we are removing the option of using sequential copies again, because it will not scale, and will create work for us again in the future.

Since we just went over how create an Axiom, let's do it now.  However, since we are working with a specific scenario and have not yet looked at all-of-operations together, we shouldn't make this a Universal Axiom, or a Production Axiom.

Instead, let's make it a Working Axiom, or a Temporary Axiom, for the purpose of this problem.

Here goes:

"Once a job that was working sequentially on a single server hits a scaling problem, and needs to be changed for performance, all later implementations of that job will not be re-implemented sequentially, even on more than a single server."

This is a little longer than I like for an Axiom, and it has more caveats than I prefer as well, as I like them simple and straight-forward, always able to be applied.

In this circumstance, we are building a Working Axiom, so it is temporary, and it needs to be precise, so it needs those caveats to fit into our problem space.

Let's review the verbiage quickly to see what caveats I baked in:

- I have limited this to implementations that already exist, and were implemented as sequential processing, on a single instance.

This could actually apply to a number of different scenarios, not just the one we are currently discussing, so this is good Axiom material, as we want to be able to apply Axioms generally, so that they are usable or actionable.

- I specify that this axiom is only to be applied once the work hits a problem.  Going around and "fixing" problems that aren't problems is a poor use of resources, as there are likely actual problems that need fixing.  Also, it is creating change in the environment, which may lead to instability or outages.  

All changes come with risk, as you only know the effects of your previous system from your history with it, once you change it you are working with a new system (new state vs. old state), so you may get different effects.  

If your change is implemented ideally, then the effects that change are exactly what you planned and wanted, so everything is OK.  If you get effects you did not want, or did not anticipate, you will likely be created a problem, which may need another change to fix.  In cases where it causes an immediate outage or degradation, it will need to be reverted or "rolled-back", if this is possible.

- Once a job gets to this "problem" state, we will not create another solution that operates sequentially, as we have proven that for this case of work (job), sequential processing doesn't scale for us.

This doesn't meant that everything should be parallelized.  Firstly, not everything can be run in parallel, as some operations are sequential operations.  If you need to get data, and then format text with it, you can't format the text at the same time you are getting data (in parallel), because you don't have the data yet.  This work requires being run sequentially.  However, if you have to process 100 of these, you may be able to run each of the 100 jobs in parallel, and then internally process them sequentially.

Writing things to work initially in sequence is a faster way to to develop, as the complexity of parallel process communication, locking and other issues can be ignored, and sequential processing is a default for many implementation languages (and all the primary Operational related languages, from any environment I've worked in).

- Finally, I state that even splitting the job across multiple servers, which may allow the sequential processing to have acceptable performance again, is not acceptable, because we are just planning to have this problem again when those N servers have to do "100" servers each, and we are back in the same spot.  This method of "throwing hardware at the problem" is frequently not the right move, although there are some special cases where it is the right move, which we will get into later.

The act of taking the same method you use now, and making a change to it that allows it to continue working, but with the knowledge that it will stop working again once you have continued growing as you have previously, I sometimes call "burying land mines in your own yard".  This is because you know this will create a problem for you in the future (problems, degradation, outages, etc), but you are doing it anyway.  You have embedded a known problem in your system, and will certainly encounter it again (unless you are going to decline and shut down the organization).

However, just because this is going to create a problem in the future, and is would be best to be avoided, there are circumstances this might be the "right thing to do".  Such as if you are working in a "Startup" organization (very small, moving very fast, not afraid to break things), and the trade offs are worth it.  

Doing things or not doing things should always be evaluated for their Engineering trade-offs, with the business goals and requirements.  Refusing to do something that is best for the organization, just because it will cause a problem in the future is also Not Engineering, even though it seems like it is "being a better Engineer", because it is not taking into account the actual environment, which is prioritizing moving fast over avoiding problems.  This will create Technical Debt, but may be worth the trade-off, just like creating Financial Debt can.

Ok, so now we have established that one method of solving this problem is to create parallel processing on the single server, and we have our Working Axiom:

"Once a job that was working sequentially on a single server hits a scaling problem, and needs to be changed for performance, all later implementations of that job will not be re-implemented sequentially, even on more than a single server."

We need at least one more method for this example to be complete, for our purposes (but not for a real world solution), and for that I will choose the method of the target servers all pulling down the deployment themselves.

This creates a "push" (parallel SCP copies) vs. "pull" (target servers pull down data) scenario, so we are comparing data moving in two different direction.

For this example, I will only provide 1 method of "pulling" the deployment data, which will be to use a Load Balancer and web servers behind the Load Balancer.

For all Production services, we should generally always have at least 2 servers for redundancy, in case one of them fails the other is available.  Note I say "generally always", which is a contradiction, but we will have to talk about this later.  However, for this example, to keep things simple we will just create a single Load Balancer and N web servers behind the Load Balancer.  For the example, we could say 1 web server, but since the purpose of Load Balancers is to distribute requests (load), we can say N.  Getting into 2 or more Load Balancers has a couple other issues I would want to bring up, so I will avoid it for this example.

So, to summarize, our "pull" alternative is:

- A Load Balancer server that accepts HTTP requests (also simplifying by not adding in security, made clear by not using HTTPS).

- N Web servers that can server the static content of our deployment data.

- Some Logic for invoking the "pull request" from the target servers, to the Load Balancer.  We will call this "Pull Logic".

- Some Logic for signaling the target servers that they should invoke the "Pull Logic".  We will this "Deploy Logic", since this invokes the deployment.


The flow of this Pull system will be:

- The new deployment data is put in place, in whatever way that is done, which is the same for any of our cases.

- Deploy Logic is triggered, by a Human running a script or initiating from a webpage, etc

- The Deploy Logic triggers the Pull Logic to get the latest data.  This could also be done several ways, but let's take the simplest one of an Agent Model, where the Deploy tells each of the 

- The Pull Logic makes an HTTP request to the Load Balancer, which proxies the request to the HTTP server, which responds with the Deployment Data

- The standard "local installation" Logic runs on the web server node, which would run in any of our cases.



